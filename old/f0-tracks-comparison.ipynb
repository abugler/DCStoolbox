{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of $f_0$ contours \n",
    "\n",
    "In this notebook we compare the results of three state-of-the-art $f_0$ extractors: pYIN, CREPE and SAC (not publicly available).\n",
    "\n",
    "#### STEPS\n",
    "\n",
    "1. Convert pYIN output to constant hopsize and unvoiced frames labeled as 0.\n",
    "2. Resample $f_0$ contours to a common timebase, i.e. SAC's timebase of 5 ms.\n",
    "3. Read three versions of contours per file and plot them together.\n",
    "4. Compute frame-wise differences between algorithms and plot them.\n",
    "5. Further steps include quantify how similar they are: similarity measures or distances. Difficult to obtain a number to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mir_eval\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_path = '../pitch/CREPE/'\n",
    "py_path = '../pitch/pYIN/'\n",
    "sac_path = '../pitch/SAC/'\n",
    "\n",
    "for fn in os.listdir(cr_path):\n",
    "    \n",
    "    joint_data = []\n",
    "    \n",
    "    print(fn)\n",
    "    \n",
    "    if not fn.endswith('csv'): continue\n",
    "        \n",
    "    sac = np.loadtxt(os.path.join(sac_path, fn[:-3]+'f0'))\n",
    "    timebase = sac[:, 0]\n",
    "    sac_fr, sac_voic = mir_eval.melody.freq_to_voicing(sac[:, 1])\n",
    "    sac_freq, _ = mir_eval.melody.resample_melody_series(sac[:, 0], sac[:, 1], sac_voic, timebase, kind='linear')\n",
    "    \n",
    "    \n",
    "    cr = np.array(pd.read_csv(os.path.join(cr_path, fn), header=None))\n",
    "    \n",
    "    # use confidence to filter out unvoiced frames, threshold=0.5\n",
    "    cr[np.where(cr[:,-1] <= 0.5)[0],1] = 0\n",
    "    cr_fr, cr_voic = mir_eval.melody.freq_to_voicing(cr[:, 1])\n",
    "    cr_freq, _ = mir_eval.melody.resample_melody_series(cr[:, 0], cr[:, 1], cr_voic, timebase, kind='linear')\n",
    "    \n",
    "\n",
    "    # a few tricks to add unvoiced frames to pyin info\n",
    "    pyi = np.array(pd.read_csv(os.path.join(py_path, fn), header=None))\n",
    "\n",
    "    fs = 22050.0\n",
    "    hopsize = 256\n",
    "    l_samples = timebase[-1] * fs\n",
    "    time_pyin = mir_eval.melody.constant_hop_timebase(hop=hopsize, end_time=l_samples) / fs\n",
    "\n",
    "    # times_pyin uses the same hopsize as the original pyin so we can directly compare them\n",
    "    pyin_new = np.zeros([len(time_pyin), 2])\n",
    "    _, _, idx_y = np.intersect1d(np.float32(pyi[:,0]), np.float32(time_pyin), return_indices=True)\n",
    "    \n",
    "    pyin_new[idx_y, 1] = pyi[:, 1]\n",
    "    pyin_new[:, 0] = time_pyin\n",
    "    \n",
    "\n",
    "    pyi_fr, pyi_voic = mir_eval.melody.freq_to_voicing(pyin_new[:, 1])\n",
    "    pyi_freq, _ = mir_eval.melody.resample_melody_series(pyin_new[:, 0], pyin_new[:, 1], \n",
    "                                                         pyi_voic, timebase, kind='linear')\n",
    "    \n",
    "    # create joint data and store\n",
    "    joint_data.append(timebase)\n",
    "    joint_data.append(cr_freq)\n",
    "    joint_data.append(pyi_freq)\n",
    "    joint_data.append(sac_freq)\n",
    "    \n",
    "    np.save(os.path.join(\n",
    "        '../pitch/joint_data', fn[:-3] + 'npy'), joint_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compare pairs of f0 tracks frame-wise by computing differences in f0\n",
    "    \n",
    "    d_cr_pyi = np.abs(\n",
    "        cr_freq - pyi_freq)\n",
    "    \n",
    "    d_cr_sac = np.abs(\n",
    "        cr_freq - sac_freq)\n",
    "    \n",
    "    d_pyi_sac = np.abs(\n",
    "        pyi_freq - sac_freq)\n",
    "    \n",
    "    '''uncomment this chunk of code for visualization and plot creation\n",
    "    \n",
    "    # VIZ\n",
    "    plt.figure(figsize=(20,15))\n",
    "    plt.subplot(311), plt.plot(timebase, cr_freq, '.r'), plt.xlabel('Time (sec)'), plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('CREPE f0 estimation', fontsize=15)\n",
    "    plt.subplot(312), plt.plot(timebase, pyi_freq, '.k'), plt.xlabel('Time (sec)'), plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('pYIN f0 estimation', fontsize=15)\n",
    "    plt.subplot(313), plt.plot(timebase, sac_freq, '.g'), plt.xlabel('Time (sec)'), plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('SAC f0 estimation', fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../pitch/figs/' + fn[:-4]+'_f0.png', format='png')\n",
    "    \n",
    "    plt.figure(figsize=(20,15))\n",
    "    plt.subplot(311), plt.plot(timebase, d_cr_pyi, '.r')#, plt.ylim([0, 70])\n",
    "    plt.xlabel('Time (sec)'), plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('CREPE-pYIN f0 difference', fontsize=15)\n",
    "    plt.subplot(312), plt.plot(timebase, d_cr_sac, '.k')#, plt.ylim([0, 70])\n",
    "    , plt.xlabel('Time (sec)'), plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('CREPE-SAC f0 difference', fontsize=15)\n",
    "    plt.subplot(313), plt.plot(timebase, d_pyi_sac, '.g')#, plt.ylim([0, 70])\n",
    "    , plt.xlabel('Time (sec)'), plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('pYIN-SAC f0 difference', fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()    \n",
    "    plt.savefig('../pitch/figs/' + fn[:-4]+'_diff.png', format='png')\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of the plots\n",
    "\n",
    "There are a few things to point out:\n",
    "* When we read data extracted by CREPE we use the third column, i.e. pitch confidence, to threshold the voiced vs. unvoiced frames. We use 0.5 by default, but there are a few passages with leakage from other voices, and in these passages this threshold reports f0 values that correspond to other microphones. This is very clear in the bass part: they don't sing in the middle of the song, but with this threshold, CREPE reports the melody sung by other voices. We need to adapt the threshold.\n",
    "\n",
    "\n",
    "* Apart from this, differences between algorithms are not very relevant in general. They increase at the beginning and end of the notes, but the stable parts of the notes are quite similar. \n",
    "\n",
    "\n",
    "* These are conclusions extracted from the plots. It would be interesting to see which are the specific parts where differences are larger. A plot with the mean of the three contours and the deviation might help detecting parts where they differ; however, we might need to solve the thresholding issue first, otherwise the deviation will peak at these places, which are not interesting for this analysis.\n",
    "\n",
    "## More plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the next chunk of code is VERY slow. Figures are already created and stored in the Drive folder.\n",
    "\n",
    "**Note**: I guess there are other ways of doing this kind of plots which are much faster to run, but for now, this is what we have.\n",
    "\n",
    "These plots correspond to the mean values between the three algorithms and are color-coded according to the standard deviation at each frame. We used four colors to easily detect the regions of the signal where the three algorithms disagree (red). \n",
    "\n",
    "When the deviation is less than 30 Hz, points are green; yellow between 30 and 70; orange between 70 and 100; and larger than 100 (strong disagreement) they are plotted in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use npy files created above\n",
    "# plotting each sample separately is SUPER SLOW but it is potentially useful to see what's happening\n",
    "'''\n",
    "npy_path = '../pitch/joint_data'\n",
    "\n",
    "for fn in os.listdir(npy_path): \n",
    "    \n",
    "    if not fn.endswith('npy'): continue\n",
    "    \n",
    "    data = np.load(os.path.join(npy_path, fn))\n",
    "    timebase, crepe, pyin, sac = data[0], data[1], data[2], data[3]\n",
    "    \n",
    "    f0s = np.concatenate([crepe[:,None], pyin[:,None], sac[:,None]], axis=1)\n",
    "    means = f0s.mean(axis=1)\n",
    "    stds = f0s.std(axis=1)\n",
    "    \n",
    "    clrs = []\n",
    "    for val in stds:\n",
    "        if val <= 30:\n",
    "            clrs.append('#2ca02c')\n",
    "        elif (val > 30) and (val <= 70):\n",
    "            clrs.append('#bcbd22')\n",
    "        elif (val > 70) and (val <= 100):\n",
    "            clrs.append('#ff7f0e')\n",
    "        elif val > 100:\n",
    "            clrs.append('#d62728')\n",
    "        else:\n",
    "            clrs.append('k')\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    i=-1\n",
    "    for m, c in zip(means, clrs):\n",
    "        i+=1\n",
    "        \n",
    "        plt.plot(timebase[i], m, '.', color=c)\n",
    "\n",
    "        \n",
    "    plt.savefig(\n",
    "        os.path.join('../pitch/joint_data', fn[:-3] + 'png'), \n",
    "        format='png')\n",
    "\n",
    "    print(\"{} plot saved\".format(fn))\n",
    "            \n",
    "'''   \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
